import os
import requests
import json
from dotenv import load_dotenv
from pprint import pprint

from openai import OpenAI


load_dotenv()

TMDB_API_KEY = os.getenv('TMDB_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=OPENAI_API_KEY)

model = 'gpt-4o-mini'


# TMDB API 함수
def get_now_playing_movies(region : str = 'KR'):
    """TMDB API를 호출하여 현재 상영 중인 영화 목록을 가져옵니다."""

    print(f'{region}의 영화를 가져옵니다.')

    url = "https://api.themoviedb.org/3/movie/now_playing"
    
    params = {
        'language' : 'ko-kr',
        'region' : region
    }
    
    headers = {
        'Authorization' : f'Bearer {TMDB_API_KEY}'
    }
    
    try:
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()    
        return response.json()['results']
    except Exception as e:
        print(e)

# pprint(get_now_playing_movies())



tools = [
    {
        "type": "function",
        "name": "get_now_playing_movies",
        "description": "현재 극장에서 상영 중인 영화 목록을 TMDB에서 가져옵니다.",
        "parameters": {
            "type": "object",
            "properties": {
                "region": {
                    "type": "string",
                    "description": "region, ISO-3166-1 code",
                },

            }
        }
    }
]


def run_conversation(user_prompt):
    
    input_list = [{"role": "user", "content": user_prompt}]

    # 1. LLM에게 1차 요청을 한다.
    response = client.responses.create(
        model=model,
        input=input_list,
        tools=tools,
    )
    input_list += response.output
    
    # 2. LLM의 응답에서 "함수 실행"에 대한 응답이 있다면 이를 실행한다.
    for item in response.output:
        if item.type == "function_call":
            if item.name == 'get_now_playing_movies':

                result = get_now_playing_movies(**json.loads(item.arguments))
                
                # 3. 함수 실행 결과를 input에 담는다.
                input_list.append({
                    "type": "function_call_output",
                    "call_id": item.call_id,
                    "output": json.dumps({
                    "result": result
                    })
                })
    # 4. 다시 한번 LLM에게 요청한다.
    response = client.responses.create(
        model=model,
        instructions="Respond only with a movies generated by a tool.",
        tools=tools,
        input=input_list,
    )

    return response.output_text
    

# 실행 예시
print(run_conversation("요즘 미국에서 볼만한 영화가 뭐가 있어?"))
